package dev.langchain4j.model.chat.response;

import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.model.chat.StreamingChatLanguageModel;

/**
 * TODO review all javadoc in this class
 * Represents a handler for streaming responses from a {@link StreamingChatLanguageModel}.
 * The handler is invoked each time the model generates a new token in a textual response.
 * If the model executes a tool instead, {@link #onComplete} will be invoked instead.
 *
 * @see StreamingChatLanguageModel
 */
public interface StreamingChatResponseHandler {
    // TODO name, make sure consistent with ChatResponse
    // TODO package

    /**
     * Invoked each time the model generates a new token in a textual response.
     * If the model executes a tool instead, this method will not be invoked; {@link #onComplete} will be invoked instead.
     *
     * @param token The newly generated token, which is a part of the complete response.
     */
    void onNext(String token);
    // TODO name (Some LLM providers batch tokens): onPartialResponse? onNextTokens?
    // TODO make sure consistent with ChatResponse and TokenStream

    /**
     * Invoked when the model has finished streaming a response.
     * If the model executes one or multiple tools,
     * this can be accessed via {@link ChatResponse#aiMessage()} -> {@link AiMessage#toolExecutionRequests()}.
     *
     * @param chatResponse The complete response generated by the model.
     *                     For textual responses, it contains all tokens from {@link #onNext} concatenated.
     */
    default void onComplete(ChatResponse chatResponse) {
    }
    // TODO name onCompleteResponse?
    // TODO make sure consistent with ChatResponse

    /**
     * This method is invoked when an error occurs during streaming.
     *
     * @param error The error that occurred
     */
    void onError(Throwable error);
}
